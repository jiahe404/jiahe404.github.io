<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[利用GitHub项目主页实现多个独立博客]]></title>
    <url>%2F2225712445.html</url>
    <content type="text"><![CDATA[之前用github page给自己搭建了一个技术博客，年后想给老爸也搞个博客，记录下他的诗歌，作为生日礼物。 现将这个摸索的过程记下。 首先明确下，一个github账户是可以实现管理多个独立博客的。这主要是利用了“项目主页”。 个人/组织主页和项目主页的区别区别见: https://help.github.com/en/articles/user-organization-and-project-pages 总结一下，github的目的是，个人/组织主页用于展示个人/组织的整体情况(于个人来讲可能是技术博客等)，而项目主页用于展示前者提到的细节、实现等(比如某一篇博客中实验的全部代码、数据集等不适合放在博客中的内容)。 前者的访问链接: &lt;username/orgname&gt;.github.io后者的访问链接: &lt;username/orgname&gt;.github.io/projectname 在项目主页的目录下，同样利用hexo搭建一个博客，就可以实现多个独立的博客了，只不过链接后缀多了个名字，也可以用于区分博客，无伤大雅。 创建项目主页repository 由于github页面也会更新升级，所以你看到的页面可能和下面的图片不太一样，不过大体上应该是相同的(本篇博客写于2019-05-26)。 1. 创建新repository 点击 new repository 输入repository名，点击 create reposityory，这里将repo命名为demo 2. 更改repository为github Pages 点击 setting 下拉找到 GitHub Pages选项卡我们找GitHub Pages选项卡，是因为需要将普通这个repo转换成github pages才能实现博客功能。红框中提示我们目前github page是禁用的，需要先添加一个内容到repo，才能将此项目作为github pages站点(也就是博客)使用。 之前版本的github是没有这步的，黄框中内容都可以直接选择，但是当前版本是置灰无法选择的。 新建文件 写入内容页面拉到最下方输入commit信息提交。 重新找到 GitHub Pages选项卡，红框中内容已改变 点击 master branch，页面稍后自动刷新，红框中提示站点已经发布 访问 https://jiahe404.github.io/demo/hello.html，显示内容！ 这一步通常要过一段时间才能生效，如果提示404，建议换个浏览器或稍等一段时间再试。 至此我们已经成功创建了一个项目主页，接下来结合hexo将此项目主页转化为一个博客！ 搭建hexo博客利用项目主页搭建博客，与个人主页相比是类似的，跳过环境问题。 1. 在本地初始化hexo博客1234cd your/blog/dirhexo inithexo ghexo s -p 8008 浏览器中看到熟悉的默认landscape主题首页: 2. 发布到github项目主页 更改站点目录下(非theme目录)_config.yml，重点！ 注意！由于使用了项目主页，链接后缀需要加上项目名称，所以url和root的配置中也要加入项目名称，本例中为demo；而deploy中的配置项，则和个人主页一致 1234567891011121314# vim _config.yml...url: http://yoursite.com/demoroot: /demo...deploy: type: git repository: https://github.com/jiahe404/demo.git branch: master... 发布 123hexo cleanhexo ghexo d 稍等几分钟后访问: https://jiahe404.github.io/demo/， 同样熟悉的landscape2： 注意到目前为止我并没有创建其他分支，如gh-pages等，一些博客资料上写个人主页只能发布master分支内容(应该是正确的)，而项目主页只能发布gh-pages分支内容(我在官方文档上并未找到依据)。 下面是官方文档的说明，只支持前半句： If your site is a User or Organization Page that has a repository named .github.ioor .github.io, you cannot publish your site’s source files from different locations. User and Organization Pages that have this type of repository name are only published from the master branch. 接下来就可以开始搞第3个博客了！ 接下来就可以开始搞第4个博客了！ 接下来就可以开始搞第N个博客了！ 没错，项目主页可以创建多个，而个人主页只能有一个，这也是二者的区别之一。 下一篇文章打算延续这个demo，写下更改主题的细节。]]></content>
      <tags>
        <tag>GitHub Pages</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python数据分析之dataframe VS sql]]></title>
    <url>%2F4015900360.html</url>
    <content type="text"><![CDATA[在分析数据时，dataframe的很多方法和sql是类似的，本文总结一些二者中的相通问题，方便互相转移，下面以mysql语法为例。 数据准备为了同时使用sql和dataframe进行分析，分别准备数据库和文本文件(下面的sql和python代码都可以直接运行)。 1. mysql表假如有一个table，包含一些影视音乐作品，数据分4列，分别为一、二级分类，作品名，评分: 123456789101112131415create table multi_category_data ( cat1 varchar(8), cat2 varchar(8), name varchar(8), score int)engine=myisam default charset=utf8;insert into multi_category_data values('影视', '电影', 'a', 5);insert into multi_category_data values('影视', '连续剧', 'b', 6);insert into multi_category_data values('影视', '连续剧', 'c', 7);insert into multi_category_data values('音乐', '流行', 'd', 10);insert into multi_category_data values('音乐', '流行', 'e', 8);insert into multi_category_data values('音乐', '民谣', 'f', 9);insert into multi_category_data values('音乐', '摇滚', 'g', 4);insert into multi_category_data values('音乐', '摇滚', 'h', 4); 2. 文本文件为了用dataframe分析，将同样的数据保存在csv_file文件中，并以dataframe格式保存到变量data:123456789101112multi_category_data = """cat1,cat2,name,score影视,电影,a,5影视,连续剧,b,6影视,连续剧,c,7音乐,流行,d,10音乐,流行,e,8音乐,民谣,f,9音乐,摇滚,g,4"""csv_file = io.StringIO(multi_category_data)data = pd.read_csv(csv_file, sep=',', names=None) 分析实战1. 去重问题 —— drop_duplicates() VS distinctdrop_duplicates()方法可以对数据按列名去重，类似于sql中的distinct。 现在我想了解作品都有哪些一、二级分类，可以如下实现: linux sort(一句话的事，但不是本文的重点) 1sort -t , -k1,1 multi_category_data sql 1234567891011select distinct cat1, cat2from multi_category_data;# 或select cat1, cat2from multi_category_datagroup by cat1, cat2; dataframe123data[['cat1', 'cat2']].drop_duplicates()# data[['cat1', 'cat2']].drop_duplicates(keep='first')# data[['cat1', 'cat2']].drop_duplicates(keep='last') 输出: drop_duplicates的参数keep指定在数据重复时保留首行或末行，可以通过第1列行号来区别保留的哪一行，如默认保留首行，所以我们看到有重复行的[影视,连续剧]、[音乐,流行]前的行号分别为1和3，对应原始数据其首次出现的行号。 2. 分组问题1. 对单列或多列执行相同的聚合操作比如想看看各个二级类目下都有多少作品: sql 123456select cat1, cat2, count(name)from multi_category_datagroup by cat1, cat2 dataframe 1data.groupby(['cat1', 'cat2'])[['name']].count() 输出: 更多例子:12345data.groupby(['cat1', 'cat2'])[['score']].max()data.groupby(['cat1', 'cat2'])[['score']].min()data.groupby(['cat1', 'cat2'])[['score']].mean()data.groupby(['cat1', 'cat2'])[['score']].sum()data.groupby(['cat1', 'cat2'])[['name', 'score']].max() 2. 对多列分别执行不同的聚合操作比如想看看所有二级分类下作品分值情况，如平均值、极值等，这里主要借助numpy的内置方法: sql 1234567select cat1, cat2, count(name), avg(score), sum(score), min(score), max(score)from multi_category_datagroup by cat1, cat2 dataframe 1data.groupby(['cat1', 'cat2']).agg(&#123;'name': [np.size], 'score': [np.mean, np.sum, np.min, np.max]&#125;) 输出: 3. 自定义聚合方法如何实现count(distinct column)操作呢？可以仿照numpy中内置的聚合方法，自定义一个:1234def my_distinct(rows): return len(set(e for e in rows))data.groupby(['cat1', 'cat2']).agg(&#123;'name': [np.size], 'score': [np.mean, np.sum, np.min, np.max, my_distinct]&#125;) 也可以采用lamba表达式实现匿名函数1data.groupby(['cat1', 'cat2']).agg(&#123;'name': [np.size], 'score': [np.mean, np.sum, np.min, np.max, lambda rows: len(set(e for e in rows))]&#125;) 输出: 4. 行转列: group_concatsql中的group_concat可以实现将同组的多行字段拼接成一列，也就是行转列，numpy.unique可以轻松实现，比如下面我们合并每个二级分类下的所有分数(例子并不很恰当，学操作就行): sql 12345678select cat1, cat2, count(name), avg(score), sum(score), min(score), max(score), count(distinct score), group_concat(score separator ',')from multi_category_datagroup by cat1, cat2 dataframe 1data.groupby(['cat1', 'cat2']).agg(&#123;'name': [np.size], 'score': [np.mean, np.sum, np.min, np.max, my_distinct, np.unique]&#125;) 输出: 先到这里，后续再补充~]]></content>
      <tags>
        <tag>python</tag>
        <tag>pandas</tag>
        <tag>dataframe</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xgboost 特征重要性]]></title>
    <url>%2F1781693973.html</url>
    <content type="text"><![CDATA[官方解释Python中的xgboost可以通过get_fscore获取特征重要性，先看看官方对于这个方法的说明: get_score(fmap=’’, importance_type=’weight’) Get feature importance of each feature. Importance type can be defined as: ‘weight’: the number of times a feature is used to split the data across all trees. ‘gain’: the average gain across all splits the feature is used in. ‘cover’: the average coverage across all splits the feature is used in. ‘total_gain’: the total gain across all splits the feature is used in. ‘total_cover’: the total coverage across all splits the feature is used in. 看释义不直观，下面通过训练一个简单的模型，输出这些重要性指标，再结合释义进行解释。 代码实践首先构造10个样例的样本，每个样例有两维特征，标签为0或1，二分类问题:123456789import numpy as npsample_num = 10feature_num = 2np.random.seed(0)data = np.random.randn(sample_num, feature_num)np.random.seed(0)label = np.random.randint(0, 2, sample_num) 输出data和label:12345678910111213# data:array([[ 1.76405235, 0.40015721], [ 0.97873798, 2.2408932 ], [ 1.86755799, -0.97727788], [ 0.95008842, -0.15135721], [-0.10321885, 0.4105985 ], [ 0.14404357, 1.45427351], [ 0.76103773, 0.12167502], [ 0.44386323, 0.33367433], [ 1.49407907, -0.20515826], [ 0.3130677 , -0.85409574]])# label:array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1]) 训练，这里为了便于下面计算，将树深度设为3(‘max_depth’: 3)，只用一棵树(num_boost_round=1): 12345import xgboost as xgbtrain_data = xgb.DMatrix(data, label=label)params = &#123;'max_depth': 3&#125;bst = xgb.train(params, train_data, num_boost_round=1) 输出重要性指标:12for importance_type in ('weight', 'gain', 'cover', 'total_gain', 'total_cover'): print('%s: ' % importance_type, bst.get_score(importance_type=importance_type)) 结果:12345weight: &#123;&apos;f0&apos;: 1, &apos;f1&apos;: 2&#125;gain: &#123;&apos;f0&apos;: 0.265151441, &apos;f1&apos;: 0.375000015&#125;cover: &#123;&apos;f0&apos;: 10.0, &apos;f1&apos;: 4.0&#125;total_gain: &#123;&apos;f0&apos;: 0.265151441, &apos;f1&apos;: 0.75000003&#125;total_cover: &#123;&apos;f0&apos;: 10.0, &apos;f1&apos;: 8.0&#125; 画出唯一的一棵树图:1xgb.to_graphviz(bst, num_trees=0) 下面就结合这张图，解释下各指标含义: weight: {‘f0’: 1, ‘f1’: 2}在所有树中，某特征被用来分裂节点的次数，在本例中，可见分裂第1个节点时用到f0，分裂第2，3个节点时用到f1，所以weight_f0 = 1, weight_f1 = 2。 total_cover: {‘f0’: 10.0, ‘f1’: 8.0}第1个节点，f0被用来对所有10个样例进行分裂，之后的节点中f0没再被用到，所以f0的total_cover为10.0，此时f0 &gt;= 0.855563045的样例有5个，落入右子树；第2个节点，f1被用来对上面落入右子树的5个样例进行分裂，其中f1 &gt;= -0.178257734的样例有3个，落入右子树；第3个节点，f1被用来对上面落入右子树的3个样例进行分裂。总结起来，f0在第1个节点分裂了10个样例，所以total_cover_f0 = 10，f1在第2、3个节点分别用于分裂5、3个样例，所以total_cover_f1 = 5 + 3 = 8。total_cover表示在所有树中，某特征在每次分裂节点时处理(覆盖)的所有样例的数量。 cover: {‘f0’: 10.0, ‘f1’: 4.0}cover = total_cover / weight，在本例中，cover_f0 = 10 / 1，cover_f1 = 8 / 2 = 4. total_gain: {‘f0’: 0.265151441, ‘f1’: 0.75000003}在所有树中，某特征在每次分裂节点时带来的总增益，如果用熵或基尼不纯衡量分裂前后的信息量分别为i0和i1，则增益为(i0 - i1)。 gain: {‘f0’: 0.265151441, ‘f1’: 0.375000015}gain = total_gain / weight，在本例中，gain_f0 = 0.265151441 / 1，gain_f1 = 75000003 / 2 = 375000015. 在平时的使用中，多用total_gain来对特征重要性进行排序。 By The Way构造xgboost分类器还有另外一种方式，这种方式类似于sklearn中的分类器，采用fit, transform形式训练模型:12345678910from xgboost import XGBClassifiercls = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.07, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=300, n_jobs=1, nthread=None, objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=True, subsample=1)# 训练模型# cls.fit(data, label) 采用下面的方式获取特征重要性指标:12for importance_type in ('weight', 'gain', 'cover', 'total_gain', 'total_cover'): print('%s: ' % importance_type, cls.get_booster().get_score(importance_type=importance_type))]]></content>
      <categories>
        <category>机器学习</category>
        <category>tree</category>
      </categories>
      <tags>
        <tag>xgboost</tag>
        <tag>特征重要性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链家小区均价数据爬取]]></title>
    <url>%2F2739431502.html</url>
    <content type="text"><![CDATA[最近有个需求，需要了解市场上小区的均价，于是试着写了个爬虫把链家上小区的信息爬取了一下。这次的爬虫任务比较简单，数据量不大，爬取链接中也没加密字段等反爬取策略，感觉还挺适合作为爬虫入门的例子。下载数据主要利用python requests库来完成，解析html页面用到xpath，下面分享些技巧和经验。 开发环境:python3.6pip install requestspip install lxml 浏览器:Chrome 1. 网站分析链家网上有小区专门的页面，域名格式为: 城市代码.lianjia.com/xiaoqu/城区代码/页码，期中页码格式为pg1、pg2这样，如果不加页码，则默认显示第1页数据。比如下面显示北京西城区的小区情况，可以看到有1716个小区，小区的名字、均价等信息都有。页面下方有页码。通过更改城市代码、城区名以及页码，就可以获取对应的小区信息。如果想要下载全国城市小区价格信息，大致分为3个步骤: 获取城市代码 获取每个城市的城区代码 获取某城区所有页码数，拼接url, 下载具体某个城市的某城区信息 下面按照3, 2, 1的顺序讲解，先把一个城区的数据搞下来，再想办法扩展到一个城市和全国范围。 2. 下载城区数据1. 定位数据位置浏览器选择Chrome，打开Chrome开发者工具(在页面上单击右键选择检查菜单)，依次点击Network栏，Doc栏，会显示本次网络请求到的页面，点击左下Name窗口中的条目，即可在右侧窗口中看到此页面的相关内容，如Headers(请求头)、Preview(预览)、Response(返回内容)、Cookies、Timing。点击Preview可以看到所需要的数据就包含在一个静态页面中，所以只要把这个页面下载下来，就可以提取其中的数据了。 爬虫任务的第一步就是要明确自己需要的数据在哪里。通常在页面上看到的早已渲染好的数据在后端会以两种方式传递给前端: 一种即是本例中，数据是早已嵌在页面中。通常这种情况数据在Doc中可以看到； 另一种情况中，页面结构和数据是分离的，数据通常以json格式通过ajax异步请求返回给客户端，再通过js程序加载到页面结构中。这种情况下数据可以在XHR栏看到。 想要定位自己所需的数据在哪，需要借助浏览器的开发者工具，分析一次请求的过程都发生了什么，先看看Doc、XHR栏里的内容，依次点点每个请求内容(Name栏中的条目)，其中Doc栏中的内容在预览情况下比较容易看，就是一个页面，而XHR栏下的通常是json格式的数据，就需要好好和页面上渲染好的数据作比较了。 2. 下载数据接下来我们就可以开始写代码爬数据了。首先定义一个Spider类:12345class Spider(object): def __init__(self, city): self.city = city self.domain = &apos;https://%s.lianjia.com&apos; % city 构造方法接收一个城市代码，在本例中即是bj，构造方法中为成员变量domain赋值。接下来定义一个download_page方法，用于下载一个页面。12345def download_page(self, url, file_out): res = requests.get(url) content = res.text with open(file_out, &apos;w&apos;) as fo: fo.write(content) 注意参数url格式为self.domain/xiaoqu/城区代码/页码。编写一个main方法运行试验下，这里先将城区和页码固定:12345678def main(): city = &apos;bj&apos; spider = Spider(city) page_file = &apos;test.html&apos; residence = &apos;xicheng&apos; page = &apos;pg2&apos; url = &apos;%s/xiaoqu/%s/%s&apos; % (spider.domain, residence, page) spider.download_page(url, page_file) 运行后可以看到当前目录下多了一个test.html 将每次网络请求的数据保存下来是个好习惯，有以下好处: 后续开发解析模块可以利用这些文件，不必重新请求数据，减少时延，节省时间，同时也能减少请求次数，防止被ban; 方便排查问题，如果程序上线了结果解析出错，可以查看文件内容，看看是网站改版了还是请求的页面错误等。 3. 解析数据1. 找到节点xpath解析用到xpath，开发者工具对xpath支持很好。先看下面的例子:按照步骤操作后可以获得一个节点的xpath。在上例中，可以获得价格的xpath:1&apos;/html/body/div[4]/div[1]/ul/li[1]/div[2]/div[1]/div[1]/span&apos; 通过开发者工具的Console栏可以方便地验证xpath:在Console栏中的命令行输入$x()方法，将上面复制的xpath粘贴进去，$x()方法接收xpath路径作为参数，返回对应的节点列表，通过[0]选择第1个节点，节点的innterText变量保存了对应的内容。通过更改xpath，就可以获取各个小区的名字和价格等信息了。 2. 优化xpath上面通过开发者工具获取的xpath是一条绝对路径，可以看到路径上有很多节点，如果以后网站改版了，路径上的某个节点改变了，那这条xpath就不能指向正确的位置了。所以通常可以利用节点的相对位置和属性等来写出更友好的xpath，比如我将上面4条xpath改写一下:同样获取到了需要的数据，而且此时看xpath也容易理解，比如第1条取小区名，是取class=”listContent”的ul下，第1个li下，class=”title”的div下，a标签的内容；如果想要取价格，就把div的class改为totalPrice，a标签换成span。这比绝对路径上一堆节点好理解多了。1234$x(&apos;//ul[@class=&quot;listContent&quot;]/li[1]//div[@class=&quot;title&quot;]/a&apos;)[0].innerText$x(&apos;//ul[@class=&quot;listContent&quot;]/li[1]//div[@class=&quot;totalPrice&quot;]/span&apos;)[0].innerText$x(&apos;//ul[@class=&quot;listContent&quot;]/li[2]//div[@class=&quot;title&quot;]/a&apos;)[0].innerText$x(&apos;//ul[@class=&quot;listContent&quot;]/li[2]//div[@class=&quot;totalPrice&quot;]/span&apos;)[0].innerText Chrome的命令行提供了很多方便的工具，$x()即是其一，利用好了事半功倍，更多知识请参见Chrome命令行。 3. python代码在Spider类中添加parse方法:1234567891011121314151617def parse(self, page_file): with open(page_file) as f: content = f.read() selector = etree.HTML(content) # 获取li数量 path = &apos;//ul[@class=&quot;listContent&quot;]/li&apos; size = len(selector.xpath(path)) # 遍历li节点 for i in range(1, size + 1): li_path = &apos;//ul[@class=&quot;listContent&quot;]/li[%s]&apos; % i # 获取小区名 path = li_path + &apos;//div[@class=&quot;title&quot;]/a&apos; name = selector.xpath(path)[0].text # 获取价格 path = li_path + &apos;//div[@class=&quot;totalPrice&quot;]/span&apos; price = selector.xpath(path)[0].text print(name, price) 运行试验下:123456789def main(): city = &apos;bj&apos; spider = Spider(city) page_file = &apos;test.html&apos; residence = &apos;xicheng&apos; page = &apos;pg2&apos; url = &apos;%s/xiaoqu/%s/%s&apos; % (spider.domain, residence, page) # spider.download_page(url, page_file) spider.parse(page_file) 输出:123456锦官苑 139610马甸南村 140382玺源台 99726黄寺大街24号院 134787新街口西里三区 111055... 需要的数据就都有了！ 3. 其他工作至于如何按页码、按城区、按城市下载全国数据，代码都已经都实现了，也不难，有时间下回分解，中秋快乐！]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在多台电脑上写GitHub Pages博客]]></title>
    <url>%2F3974204864.html</url>
    <content type="text"><![CDATA[之前在公司的mac上写过GitHub Pages，最近离职了，于是想用自己的windows本继续写。本以为安装好nodejs，npm等环境再git pull一下就可以在windows本上写博客，实践了才发现一些问题。git pull下来的内容根本没办法直接在本地显示博客内容，因为少了很多配置等文件。下图分别是mac上(已经拷贝到windows本)可以运行的博客和在windows本上pull下来的文件： mac上可以运行的博客 git上pull下来的文件 可以看到在git上的内容少了几个目录，如node_modules(node.js执行需要的库)、scaffolds(生成md文件时用到的模板，可自定义: hexo new [post/page/draft] )、source(自己写的md文件)、themes(主题)，以及package.json、package-lock.json等文件。 之前对node.js，hexo这套东西也不太了解，通过这次折腾算是多了些理解。GitHub Pages保存的内容顾名思义，就是一些html页面，以及支持这些页面显示的字体(fonts)、图片(img), 标签(tags)、js、css等，在执行hexo d命令时，以上的内容会提交到git仓库的master分支上；而支撑生成这些静态页面的md文件(scaffolds、source)，主题(themes)，以及在本地调试用到的node.js环境(node_modules、package.json、package-lock.json)则不会被提交。 知道了这些就可以利用git的分支解决这个问题了： 支撑生成这些静态页面的md文件(scaffolds、source)，主题(themes)等是需要保存的，可以提交到新的分支上； 本地调试用到的node.js环境(node_modules)就不必提交了，可能各个电脑安装的版本也不一样，需要时 npm install 一条命令就生成了。 由于我们是从已有博客恢复环境，而不是从头开始创建环境，所以 hexo init 命令不需要执行，此命令会初始化hexo所需环境及基本配置，下图即是运行后生成的目录结构。但以下目录及文件都是已有的(在你的另一台电脑的硬盘上，而不是git仓库)，可以直接拷贝过来。但是我建议node_modulds就别拷贝了，可能不同电脑上版本不同。但是package.json、package-lock.json是需要提交的，其中保存了hexo及package的版本等信息，执行npm install 需要这两个文件。这里要注意执行npm install 后，由于两台电脑上hexo及package版本不一致，这两个文件也可能会被改写，所以如果需要频繁地在多台电脑上写博客，这两个文件可以不再提交。 123456# 创建新分支保存md文件及主题等git checkout -b hexo# 编辑.gitignore文件，在新分支上忽略node_modules等vi .gitignore# 提交需要保存的文件git add ... git commit ... git push origin hexo 以后换了新电脑，执行下面的命令：12345678910# 克隆代码库git clone your/github/repo dir/for/blog# 切换分支cd dir/for/bloggit checkout hexo# 安装node.js环境npm install# 测试hexo ghexo s 以后写md、调主题就在hexo分支上进行，记得最后都git add … git commit … git push origin hexo，以便保存到git仓库。在hexo分支上运行hexo d就可以，实际上是将hexo g生成的静态pages推送到主分支上(这些操作都由 hexo-deployer-git工具完成，对我们来讲是透明的)。]]></content>
      <tags>
        <tag>GitHub Pages</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python2 编码问题]]></title>
    <url>%2F2385407591.html</url>
    <content type="text"><![CDATA[一些定义 字符(character)字符是文字的最小的组成单位，其为一种抽象定义(不要与 java 或 c 中的 char 类型混淆，后者为特定计算机语言的数据类型)，取决于语言或是上下文环境，比如’A’, ‘a’为英文中的字符，’纺’,’织’是汉语中的字符(注意绞丝旁并不能称为是一个字符，因为在汉语中，它无法单独成为一个字) 。 图像字符(glyph)人们在交谈时通过独特的发音来表达一个字符，而当在屏幕或是纸面上时，则是通过一些特定图形来表达一个字符。比如在汉字中用一条横线来表示’一’这个字符。这个图像化的形象即被称为图像字符，在计算机中可以通过矢量图表示。 字符串(characters/string)字符串由若干字符组成的串。 码点(code point)码点也有译为码位(code position)，是一个整数，常用16进制表示。Unicode标准就是维护了一张字符与码点的映射表，说白了，就是将一个字符用某个唯一的整数表示，这样全世界的计算机上都保存这样一张表，数据传输时只需要传输一堆数字就好，再用这张表去解析，找到对应的字符即可，而无需去传输字符所对应的矢量图。这就是标准的作用。下图是’一’的 Unicode 编码示例(图片源自Charbase): 既然有了 Unicode，为什么还有一众编码方式？现在我们知道了，世界上有这么一个统一的标准，那为什么还有 latin1, utf8, gbk 这些编码方式呢？为何不只用这一种编码方式，也省去了 decode, encode 的麻烦了。其实这是概念上的混淆。如果你还不知道自己错在哪了，请思考一个问题，作为程序员，我们都或多或少地了解 MVC 模式，那我想问，既然 MVC 这么好，为什么还要用 MFC(vc++框架), struts(java web 框架), ci(php 框架)呢？相信你可以理解这个荒唐的问题。 MVC 是一种设计模式，一种思想，将 model，view, controller 解耦。而上面提到的几种框架，其实是对这种设计模式的针对不同的场景进行的实现。可以将 MVC当作一个类，而具体干活的，其实是需要 new 一个对象出来，也就是MFC等一众对 MVC 思想的具体实现。 与 MVC 这个问题类似，Unicode 其实是多种编码体系(如ISO/IEC 10646)中的一种，而具体实现这种体系的，则为 UTF(Unicode Transformation Format)，包括 utf8, utf16, utf32等编码方式。 Unicode标准的确维护了一张字符与码点的映射表，所以你可以查看某个字符的 Unicode 编码，正如在上面’一’的 Unicode 编码示例图中可见其 Unicode 编码为’\u4e00’。然而对于码点在网络传输或物理存储具体要如何操作，如码点前置0怎么处理，用多少字节表示一个字符等问题，Unicode标准并未规定，具体是由UTF来实现的。 写到这里，上面提到的所有概念和内容与python没有半毛钱关系，对于任何编程语言上面的概念都是放诸四海皆准的。上文所提到的 Unicode 全部指代 Unicode 标准，千万不要和 python 中的 unicode 类型混淆(注意两个概念在本文中用首字母的大小写来区别)！ 每种编码方式都有各自的特点，如汉字’一’用 gbk 及 utf16 编码占2个字节，用 utf8编码则占3个字节，’a’用 utf8编码则只占一个字节，用 utf16仍然占用2个字节，而 Unicode 统一使用4个字节来表示所有字符，但是对于定长的东西，在内存中寻址就很快很方便，这也是在计算机内存中统一使用Unicode，而当要保存或传输数据时，转换成 UTF 编码(当然也可以采用其他编码)的原因。 gbk是针对汉字的编码方式，虽然可以用较少的字节来表示一个汉语字符，但对于很多其他国家的字符就无能为力了，而 utf 编码则基本囊括了世界上所有语言的字符集；utf16，utf32分别用2、4个字节表示一个字符，然而对于英文等语言环境来讲，一个字节完全可以表示一个字符，就会浪费存储空间。 不同的编码方式都有其诞生及存在的合理性，不能只凭字符占用字节数，能表达的语言种类或是大小端处理等来衡量优劣，适合的就是最好的。所以，我们无法用一种编码方式走遍天下，那就得好好了解下如何在多种编码方式中处理数据，下面就来看下 python 中对于字符编码的处理。 Python2中编码的转换在python2中，字符串可以用 unicode 或 str 这两种类型来表示。unicode类型的字符串由若干Unicode字符组成，本文中称之为unicode串，注意 unicode 与 unicode串是类与实例的关系，如下面代码中，a，b，c，d都是unicode串，a，b，c，d 的类型都是unicode：12345678# -*- coding: utf8 -*-a = u&apos;a&apos;b = u&apos;一&apos;c = u&apos;一a&apos;d = u&apos;一a\u4e00&apos; # 汉字&apos;一&apos;的 unicode 编码为\u4e00e = u&apos;一a\u4e00\x61\141&apos;print a, b, c, d, eprint type(a), type(b), type(c), type(d), type(e) 输出：12a 一 一a 一a一 一a一aa&lt;type &apos;unicode&apos;&gt; &lt;type &apos;unicode&apos;&gt; &lt;type &apos;unicode&apos;&gt; &lt;type &apos;unicode&apos;&gt; &lt;type &apos;unicode&apos;&gt; 如果对变量 e 的内容有疑问，可以参考这篇文章 str类型的字符串其实为字节序列，本文中称之为str串，一个str串的编码方式是需要指定的，这点和 unicode 串很不一样。因为 unicode 串一定是由 Unicode字符组成，这个规则是确定的。但是 python 不知道一个 str 串是采用了何种编码方式，只能使用默认的编码方式来处理，这个过程就有可能出错，事实上关于字符串转码的问题大多就出现在这里。 在 python2 中，将 unicode串转换成 str串称为编码(encode), 而将str串转换成 unicode串称之为解码(decode)。 编码(encode):将内存中的unicode串保存到外存时，需要将之编码(encode)为 str串(即字节序列)，看下面的例子：1234# # -*- coding: utf8 -*-one = u&apos;一&apos;with open(&apos;one.txt&apos;, &apos;w&apos;) as f: f.write(one) 运行报错：1UnicodeEncodeError: &apos;ascii&apos; codec can&apos;t encode character u&apos;\u4e00&apos; in position 0: ordinal not in range(128) 意思是说采用 ascii 编解码标准无法对’一’进行编码，这是因为f.write(one)这行代码在执行时被解释成f.write(one.encode(‘ascii’))。python 默认会采用 ascci 编解码标准对unicode串进行编码，而汉字’一’的Unicode 编码为4e00(16进制)，超出了 ascii 编解码标准的上限128，所以报错。 正确做法是在将变量保存至外存时，显式地指定恰当的编码方式：1234# -*- coding: utf8 -*-one = u&apos;一&apos;with open(&apos;one.txt&apos;, &apos;w&apos;) as f: f.write(one.encode(&apos;utf8&apos;)) 解码(decode):当需要将不同编码类型的字符串放在一起处理时(比如从数据库中读入一个字段，在内存中其为一个unicode串，将之与一个 utf8编码的文件中的第一行数据拼接成一个字符串)，最好将 str串解码成unicode串，看下面的代码示例：1234567# -*- coding: utf8 -*-database_field = u&apos;一&apos;file_first_line = &apos;二&apos;print database_field, type(database_field)print file_first_line, type(file_first_line)print database_field \ + file_first_line # 分行是为方便查看是哪行转换出错 运行报错：123456一 &lt;type &apos;unicode&apos;&gt;二 &lt;type &apos;str&apos;&gt;Traceback (most recent call last): File &quot;/Users/baidu/PhpstormProjects/scripts_dev/python/grammer/coding.py&quot;, line 7, in &lt;module&gt; + file_first_line # 分行是为方便查看是哪行转换出错UnicodeDecodeError: &apos;ascii&apos; codec can&apos;t decode byte 0xe4 in position 0: ordinal not in range(128) 可以看到单独处理（输出）两个变量是没问题的，不过放在一起就完蛋了。从报错信息可以看到是对file_first_line按照ascii decode 时出错，原因不难理解，unicode串与 str串类型不一致，无法直接拼接，python 在尝试将 str串解码为 unicode 串时，采用了错误的 ascii 编解码标准。 可以通过将二者统一转换成一种类型来解决：123456789# -*- coding: utf8 -*-database_field = u&apos;一&apos;file_first_line = &apos;二&apos;print database_field, type(database_field)print file_first_line, type(file_first_line)print database_field + file_first_line.decode(&apos;utf8&apos;), \ type(database_field + file_first_line.decode(&apos;utf8&apos;))print database_field.encode(&apos;utf8&apos;) + file_first_line, \ type(database_field.encode(&apos;utf8&apos;) + file_first_line) 运行如下：1234一 &lt;type &apos;unicode&apos;&gt;二 &lt;type &apos;str&apos;&gt;一二 &lt;type &apos;unicode&apos;&gt;一二 &lt;type &apos;str&apos;&gt;]]></content>
      <tags>
        <tag>python</tag>
        <tag>编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows 10 下安装Tensorflow GPU版]]></title>
    <url>%2F2673703756.html</url>
    <content type="text"><![CDATA[之前在自己的windows上安装了tensorflow1.0.1-CPU版，后来想用gpu进行计算，于是安装gpu版。没想到软件之间的依赖关系、版本等导致数种问题，百度谷歌良久才调通程序，特记下曲折的安装过程和一些细节，尽量解释选择软件版本的原因，希望能减轻读者的痛苦。 我的环境 操作系统：Windows 10 64bit 显卡型号：GeForce GTX 950M 软件准备 Anaconda3-4.2.0-Windows-x86_64.exe Anaconda是一个用于科学计算的Python发行版，安装后，python以及常用的用于计算的package如numpy、matplotlib、Pillow也都安装好了，而且还能切换不同版本的python，很方便。 安装tensorflow是不必安装Anaconda的，但是你至少要有python环境，而且需要是python3.5.x，因为正式1.0版的tensorflow不支持python2.x版。Anaconda3中内置的python为3.x版本（Anaconda2中内置的python为2.x版本），不过最新的Anaconda内置python为3.6版，为了避免潜在的麻烦，我安装了较新的历史版本（没错，Anaconda可以切换python版本，不过我之前在Anaconda2下安装python3后出现了一些问题，没时间较真了）。 Anaconda 官网历史版本下载 一篇不错的入门教程 tensorflow_gpu-1.0.1-cp35-cp35m-win_amd64.whl tensorflow在github的提供了以下几个版本： Linux CPU-only: Python 2 / Python 3.4 / Python 3.5 Linux GPU: Python 2 / Python 3.4 / Python 3.5 Mac CPU-only: Python 2 / Python 3 Mac GPU: Python 2 / Python 3 Windows CPU-only: Python 3.5 64-bit Windows GPU: Python 3.5 64-bit Android: demo APK, native libs DirectX SDK 官网下载 VS2015/2013/2012 安装cuda前必须先安装一款VS，具体安装哪款，请参照下面对于cuda的介绍。 cudnn-8.0-windows10-x64-v5.1 针对深度神经网络采用GPU计算的加速库，下载解压后你会得到3个.dll文件。 官网下载（下载需要注册，并且回答一些问题） 安装 cuda cuda是一种并行计算平台，可以利用gpu进行并行计算。请注意，tensorflow1.x-gpu版只支持cuda8.x版本，你可以在这里确认这个问题 操作系统对cuda8的支持（常用的系统应该都没问题，即使在cross情况下——32位的系统在64位机器上）: Operating System Native x86_64 Cross (x86_32 on x86_64) Windows 10 YES YES Windows 8.1 YES YES Windows 7 YES YES Windows Server 2012 R2 YES NO Windows Server 2008 R2 DEPRECATED YES YES VS对cuda8的支持(注意cross情况): Compiler IDE Operating System Native x86_64 Cross (x86_32 on x86_64) Visual C++ 14.0 Visual Studio 2015 YES YES Visual C++ 14.0 Visual Studio Community 2015 YES NO Visual C++ 12.0 Visual Studio 2013 YES YES Visual C++ 11.0 Visual Studio 2012 YES YES Visual C++ 10.0 DEPRECATED Visual Studio 2010 YES YES cuda官网安装教程（这里有上面两张表格） 我之前就不幸地安装了唯一一种不支持的情况，安装Visual Studio Community 2015 32bit在我64bit笔记本上，报了一堆错，卸载后安装vs2013后解决问题。到这里你就可以根据自己的情况安装对应的VS了。 官网下载 一些问题 cudnn是必要的我之前参考了一些教程，以为cudnn只是为了提速，并不是必须的，就没有安装，结果出错。在tensorflow官网安装教程中，要求cudnn是必装的，并且要将动态链接库文件添加到环境变量中，我照做后仍报错，后来参照这篇教程将3个.dll文件拷贝到duda安装目录下对应路径下，解决问题。]]></content>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello Blog]]></title>
    <url>%2F3393309510.html</url>
    <content type="text"><![CDATA[打算搭建个博客，记录学习经验。想想还有点小激动呢^_^]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>
